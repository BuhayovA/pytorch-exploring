{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:07:40.956117Z",
     "start_time": "2024-10-02T17:07:40.183241Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install torch torchvision torchaudio matplotlib numpy tqdm",
   "id": "784d61311e18434e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.4.1)\r\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.19.1)\r\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.12/site-packages (2.4.1)\r\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.12/site-packages (3.9.2)\r\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (2.1.1)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (4.66.5)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.12/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2024.9.0)\r\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch) (75.1.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.12/site-packages (from torchvision) (10.4.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.12/site-packages (from matplotlib) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.12/site-packages (from matplotlib) (4.54.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from matplotlib) (24.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib) (3.1.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\r\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-02T17:07:40.965922Z",
     "start_time": "2024-10-02T17:07:40.961458Z"
    }
   },
   "source": [
    "# base\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "# helpers\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:07:41.337647Z",
     "start_time": "2024-10-02T17:07:41.335612Z"
    }
   },
   "cell_type": "code",
   "source": "plt.style.use('dark_background')",
   "id": "65fc9ca257d0252e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:07:41.834320Z",
     "start_time": "2024-10-02T17:07:41.832098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 50\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')\n",
    "print(f'Epoch: {EPOCHS}')"
   ],
   "id": "1ff8f6ccec645ff3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Epoch: 50\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:07:42.315637Z",
     "start_time": "2024-10-02T17:07:42.312371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.len_dataset = 0\n",
    "        self.data_list = []\n",
    "        \n",
    "        for path_dir, dir_list, file_list in os.walk(path): # os.walk is GENERATOR func\n",
    "            # print(f'[WALK]: {path_dir} | {dir_list} | {len(file_list)}')\n",
    "            \n",
    "            if path_dir == path:\n",
    "                self.classes = sorted(os.listdir(path_dir))\n",
    "                self.class_to_index = { cls_name: i for i, cls_name in enumerate(self.classes) }\n",
    "                continue\n",
    "            \n",
    "            cls = path_dir.split(os.sep)[-1]\n",
    "\n",
    "            for file_name in file_list:\n",
    "                file_path = os.path.join(path_dir, file_name)\n",
    "                self.data_list.append((file_path, self.class_to_index[cls]))\n",
    "                \n",
    "            self.len_dataset += len(file_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_dataset\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_path, target = self.data_list[index]\n",
    "        sample = Image.open(file_path)\n",
    "        \n",
    "        # print(f'[PIL => Sample]: {sample}')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample, target"
   ],
   "id": "b099ff99643635b",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:07:42.920938Z",
     "start_time": "2024-10-02T17:07:42.821608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.5, ), std=(0.5, ))\n",
    "])\n",
    "\n",
    "train_dataset = MNISTDataset(path='./mnist/training', transform=transform)\n",
    "test_dataset = MNISTDataset(path='./mnist/testing', transform=transform)"
   ],
   "id": "ae3c489e23d758ee",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:07:43.371094Z",
     "start_time": "2024-10-02T17:07:43.367340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset_split, val_dataset = random_split(train_dataset, [0.7, 0.3])\n",
    "print(f'[Train]: {len(train_dataset_split)}')\n",
    "print(f'[Validation]: { len(val_dataset)}')\n",
    "print(f'[Test]: {len(test_dataset)}')"
   ],
   "id": "342f900fe71b4518",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]: 42000\n",
      "[Validation]: 18000\n",
      "[Test]: 10000\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:07:44.041973Z",
     "start_time": "2024-10-02T17:07:44.038193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset_split, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "aba6e63669bf2c3e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:07:44.447907Z",
     "start_time": "2024-10-02T17:07:44.445628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_size, 128)\n",
    "        self.layer_2 = nn.Linear(128, output_size)\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.act(x)\n",
    "        out = self.layer_2(x)\n",
    "        \n",
    "        return out\n"
   ],
   "id": "462fa0115441ecfb",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:07:45.357892Z",
     "start_time": "2024-10-02T17:07:45.354838Z"
    }
   },
   "cell_type": "code",
   "source": "model = MyModel(input_size=784, output_size=10).to(device)",
   "id": "63657ee0e5c1f8b2",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:17:24.545061Z",
     "start_time": "2024-10-02T17:17:24.539571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# input = torch.rand([16, 784], dtype=torch.float32).to(device)\n",
    "# \n",
    "# out = model(input)\n",
    "# out.shape"
   ],
   "id": "7f23cdf450df49c2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:17:25.168065Z",
     "start_time": "2024-10-02T17:17:25.165610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_model = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "9cde8d345fbda9c3",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # model training\n",
    "    model.train()\n",
    "    running_train_loss = []\n",
    "    true_answer_counter = 0\n",
    "    train_loop = tqdm(train_loader, leave=False)\n",
    "    for x, targets in train_loop:\n",
    "        x = x.reshape(-1, 28*28).to(device)\n",
    "        targets = targets.reshape(-1).to(torch.int32)\n",
    "        targets = torch.eye(10)[targets].to(device)\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = loss_model(pred, targets)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        running_train_loss.append(loss.item())\n",
    "        mean_train_loss = sum(running_train_loss) / len(running_train_loss)\n",
    "        \n",
    "        true_answer_counter += (pred.argmax(dim=1) == targets.argmax(dim=1)).sum().item()\n",
    "        \n",
    "        train_loop.set_description(f'Epoch [{epoch+1}/{EPOCHS}], train_loss_avg: {mean_train_loss:.4f}')\n",
    "\n",
    "    running_train_acc = true_answer_counter / len(train_dataset)\n",
    "    \n",
    "    train_loss.append(mean_train_loss)\n",
    "    train_acc.append(running_train_acc)\n",
    "    \n",
    "    # model validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_val_loss = []\n",
    "        true_answer_counter = 0\n",
    "        for x, targets in val_loader:\n",
    "            x = x.reshape(-1, 28*28).to(device)\n",
    "            targets = targets.reshape(-1).to(torch.int32)\n",
    "            targets = torch.eye(10)[targets].to(device)\n",
    "            \n",
    "            pred = model(x)\n",
    "            loss = loss_model(pred, targets)\n",
    "            \n",
    "            running_val_loss.append(loss.item())\n",
    "            mean_val_loss = sum(running_val_loss) / len(running_val_loss)\n",
    "            \n",
    "            true_answer_counter += (pred.argmax(dim=1) == targets.argmax(dim=1)).sum().item()\n",
    "    \n",
    "        running_val_acc = true_answer_counter / len(train_dataset)\n",
    "        \n",
    "        val_loss.append(mean_val_loss)\n",
    "        val_acc.append(running_val_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}], train_loss: {mean_train_loss:.4f}, train_acc: {running_train_acc:.4f}, val_loss: {mean_val_loss:.4f}, val_acc: {running_val_acc:.4f}')"
   ],
   "id": "10f4deedeabb6895",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.show()"
   ],
   "id": "a795e7b0ddcdc289",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.legend(['train_acc', 'val_acc'])\n",
    "plt.show()"
   ],
   "id": "4be4a5f77016f25b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8e5817f227a902ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
